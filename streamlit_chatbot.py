#!/usr/bin/env python3
"""
EigenAI Interactive Chatbot - v1.2.0

Chat with a recursive self-modifying AI that builds understanding through eigenstate detection.
Watch in real-time as the AI's processing framework evolves during conversation.

New in v1.2.0: Context Accumulation Layer with novelty detection
New in v1.1.0: F-aware parallel tokenization (3-30Ã— speedup)
"""

import streamlit as st
import numpy as np
import heapq
import sys
import os

# Dynamic path resolution (fixes hardcoded path issues)
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from src.eigen_recursive_ai import RecursiveEigenAI
from src.eigen_discrete_tokenizer import tokenize_word, xor_states, compute_change_stability

def tokenize_text(text):
    """Tokenize a text string into discrete tokens"""
    words = text.split()
    return [tokenize_word(word) for word in words]

def tokenize_and_record_transitions(text, learned_tokens):
    """
    Tokenize text and record transition statistics for each token

    Parameters
    ----------
    text : str
        Input text
    learned_tokens : dict
        Dictionary of learned tokens (will be updated in-place)

    Returns
    -------
    tokens : list of DiscreteToken
        Tokens with updated transition statistics
    """
    words = text.split()
    tokens = []

    # Initialize state for XOR cascade
    state = (0, 0, 0, 0)

    for word in words:
        # Get or create token
        word_key = word.lower()
        if word_key in learned_tokens:
            token = learned_tokens[word_key]
        else:
            token = tokenize_word(word)

        # Compute transition caused by this token
        prev_state = state
        state = xor_states(state, token.as_tuple())
        C, S, ds2 = compute_change_stability(prev_state, state)

        # Record transition statistics
        token.record_transition(ds2)

        # Update learned tokens
        learned_tokens[word_key] = token
        tokens.append(token)

    return tokens

def tokenize_and_record_transitions_parallel(text, learned_tokens, F=8):
    """
    F-aware parallel tokenization applying batching framework: k* = âˆš(oPÂ·F/c)

    Reduces processing depth from O(n) to O(n/F + log_F(n)) by:
    1. Batch token lookup/creation (F words at once)
    2. Vectorized transition computation (numpy arrays)
    3. Hierarchical state reduction

    For n=1000 words, F=8:
    - Old depth: 1000 sequential operations
    - New depth: 125 batches + 3 levels â‰ˆ 128 operations (7.8Ã— reduction)

    Parameters
    ----------
    text : str
        Input text
    learned_tokens : dict
        Dictionary of learned tokens (will be updated in-place)
    F : int
        Fan-in capacity (batch size). Default=8 for human-comprehension tasks.
        Increase to 32-64 for large article processing.

    Returns
    -------
    tokens : list of DiscreteToken
        Tokens with updated transition statistics
    """
    words = text.split()
    if not words:
        return []

    tokens = []
    word_keys = [w.lower() for w in words]

    # PHASE 1: Batch token lookup/creation (F words at once)
    # Reduces database access patterns from O(n) to O(n/F)
    for i in range(0, len(words), F):
        batch_words = words[i:i+F]
        batch_keys = word_keys[i:i+F]

        # Parallel lookup: check all F words at once
        batch_tokens = []
        for word, word_key in zip(batch_words, batch_keys):
            if word_key in learned_tokens:
                token = learned_tokens[word_key]
            else:
                # Create new token (parallelizable across batch)
                token = tokenize_word(word)
                learned_tokens[word_key] = token
            batch_tokens.append(token)

        tokens.extend(batch_tokens)

    # PHASE 2: Vectorized XOR cascade and transition computation
    # Convert all tokens to numpy arrays for parallel computation
    n = len(tokens)
    L_vals = np.array([t.L for t in tokens], dtype=np.int32)
    R_vals = np.array([t.R for t in tokens], dtype=np.int32)
    V_vals = np.array([t.V for t in tokens], dtype=np.int32)
    M_vals = np.array([t.M for t in tokens], dtype=np.int32)

    # Compute XOR cascade: state[i] = state[i-1] âŠ• token[i]
    # This must be sequential due to dependencies, but XOR is fast
    states_L = np.zeros(n+1, dtype=np.int32)
    states_R = np.zeros(n+1, dtype=np.int32)
    states_V = np.zeros(n+1, dtype=np.int32)
    states_M = np.zeros(n+1, dtype=np.int32)

    for i in range(n):
        states_L[i+1] = states_L[i] ^ L_vals[i]
        states_R[i+1] = states_R[i] ^ R_vals[i]
        states_V[i+1] = states_V[i] ^ V_vals[i]
        states_M[i+1] = states_M[i] ^ M_vals[i]

    # PHASE 3: Vectorized transition computation (parallelized)
    # Compute C, S, dsÂ² for all transitions at once
    prev_states = np.stack([states_L[:-1], states_R[:-1], states_V[:-1], states_M[:-1]], axis=1)
    curr_states = np.stack([states_L[1:], states_R[1:], states_V[1:], states_M[1:]], axis=1)

    # XOR to find changed bits
    changed = prev_states ^ curr_states

    # Count bits: stable = 1024 - changed, changed = changed
    # Using numpy's binary operations for parallel bit counting
    C_vals = np.array([bin(changed[i, 0]).count('1') + bin(changed[i, 1]).count('1') +
                       bin(changed[i, 2]).count('1') + bin(changed[i, 3]).count('1')
                       for i in range(n)])
    S_vals = 32 - C_vals  # 4 bytes Ã— 8 bits = 32 total bits
    ds2_vals = S_vals**2 - C_vals**2

    # PHASE 4: Batch record transitions (vectorized)
    for i, (token, ds2) in enumerate(zip(tokens, ds2_vals)):
        token.record_transition(int(ds2))

    return tokens

st.set_page_config(
    page_title="EigenAI Chatbot",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("ğŸ§  EigenAI: Understanding Through Eigenstate Detection")
st.markdown("""
Chat with an AI that measures its own understanding through geometric eigenstate convergence.
Watch the metrics panel to see how the AI's processing framework evolves.
""")

# Settings bar
col_settings = st.columns([3, 1])
with col_settings[1]:
    entropy_mode = st.toggle(
        "âœ¨ Entropy Weighting",
        value=False,
        help="Weight semantics by information density (14.6Ã— better orthogonality)"
    )

if "ai" not in st.session_state:
    st.session_state.ai = RecursiveEigenAI(embedding_dim=64)
    st.session_state.messages = []
    st.session_state.metrics_history = []
    st.session_state.learned_tokens = {}

def compute_semantic_similarities_vectorized(tokens_list, avg_L, avg_R, avg_V, avg_M):
    """
    Vectorized semantic similarity computation using numpy

    10-100x faster than Python loops for large token sets

    Returns: list of (similarity_score, word, token) tuples
    """
    if not tokens_list:
        return []

    # Extract LRVM values into numpy arrays
    L_vals = np.array([t.L for _, t in tokens_list])
    R_vals = np.array([t.R for _, t in tokens_list])
    V_vals = np.array([t.V for _, t in tokens_list])
    M_vals = np.array([t.M for _, t in tokens_list])

    # Vectorized similarity computation (all tokens at once)
    l_sim = 1.0 / (1.0 + np.abs(L_vals - avg_L) / 255.0)
    r_sim = 1.0 / (1.0 + np.abs(R_vals - avg_R) / 255.0)
    v_sim = 1.0 / (1.0 + np.abs(V_vals - avg_V) / 255.0)
    m_sim = 1.0 / (1.0 + np.abs(M_vals - avg_M) / 255.0)

    # Weighted average (M has 2x weight as in original)
    similarities = (l_sim + r_sim + v_sim + 2.0 * m_sim) / 5.0

    # Return as list of (similarity, word, token) for heapq
    return [(sim, word, token) for sim, (word, token) in zip(similarities, tokens_list)]

def generate_token_response(learned_tokens, user_message, max_words=10):
    """
    Generate response using geometric classification of tokens (OPTIMIZED)

    Optimizations:
    - Numpy vectorization for semantic similarity (10-100x faster)
    - heapq for top-N selection (O(n log k) instead of O(n log n))

    Uses transition statistics to separate:
    - Time-like tokens (S > C): structural/sequential words
    - Space-like tokens (C > S): semantic/content words
    - Light-like tokens (C = S): relational/transformational words
    """
    if len(learned_tokens) < 3:
        return "â‹¯ (Not enough tokens learned yet to generate response)"

    # Classify tokens by their geometric properties
    time_like_tokens = []  # Structural: the, to, is, of, etc.
    space_like_tokens = []  # Semantic: cat, quantum, oscillation, etc.
    light_like_tokens = []  # Relational: is, becomes, connects, etc.

    for word, token in learned_tokens.items():
        classification = token.get_classification()
        if classification == 'time-like':
            time_like_tokens.append((word, token))
        elif classification == 'space-like':
            space_like_tokens.append((word, token))
        elif classification == 'light-like':
            light_like_tokens.append((word, token))

    # Tokenize user input to extract semantic intent
    user_tokens = tokenize_text(user_message.lower())
    if not user_tokens:
        return "â‹¯"

    # Compute average semantic pattern from user input
    avg_L = sum(t.L for t in user_tokens) / len(user_tokens)
    avg_R = sum(t.R for t in user_tokens) / len(user_tokens)
    avg_V = sum(t.V for t in user_tokens) / len(user_tokens)
    avg_M = sum(t.M for t in user_tokens) / len(user_tokens)

    # Vectorized similarity computation for space-like tokens
    space_similarities = compute_semantic_similarities_vectorized(
        space_like_tokens, avg_L, avg_R, avg_V, avg_M
    )

    # Target distribution based on natural language
    num_structural = max(1, max_words // 4)  # ~25% structural
    num_content = max(1, max_words // 2)     # ~50% content
    num_relational = max(1, max_words // 4)  # ~25% relational

    # Use heapq to get top N without sorting entire list (O(n log k) vs O(n log n))
    top_space_tokens = heapq.nlargest(num_content, space_similarities, key=lambda x: x[0])

    # Build response with geometric structure
    response_words = []

    # Add time-like tokens (structural/sequential)
    for word, token in time_like_tokens[:num_structural]:
        response_words.append(word)

    # Add space-like tokens (semantic content) - highest similarity first
    for sim, word, token in top_space_tokens:
        response_words.append(word)

    # Add light-like tokens (relational/transformational)
    for word, token in light_like_tokens[:num_relational]:
        response_words.append(word)

    if not response_words:
        return "â‹¯"

    return " ".join(response_words)

col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("ğŸ’¬ Conversation")

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            if "metrics" in msg:
                with st.expander("ğŸ“Š Understanding Metrics & Tokens"):
                    m = msg["metrics"]
                    col_a, col_b, col_c = st.columns(3)
                    with col_a:
                        st.metric("Eigenstate", "âœ“" if m.get("eigenstate") else "âœ—")
                    with col_b:
                        st.metric("Iteration", m.get("iteration", "N/A"))
                    with col_c:
                        st.metric("Vocab Size", m.get("vocab_size", 0))

                    # Show discrete tokens if available
                    if "tokens" in m:
                        st.divider()
                        st.caption("ğŸ”¢ Discrete Token Bit Patterns & Classification")
                        for token in m["tokens"][:5]:  # Show first 5 tokens
                            classification = token.get_classification()
                            ratios = token.get_classification_ratios()

                            # Icon for classification
                            icon = "â±ï¸" if classification == "time-like" else "ğŸŒ" if classification == "space-like" else "ğŸ’«" if classification == "light-like" else "â“"

                            st.code(f"{token.word:>12} â†’ L:{token.L:08b} R:{token.R:08b} V:{token.V:08b} M:{token.M:08b}", language="text")
                            st.caption(f"{icon} {classification} | Usage: {token.usage_count} | T:{ratios['time-like']:.2f} S:{ratios['space-like']:.2f} L:{ratios['light-like']:.2f}")

                    if m.get("entropy_weighted"):
                        st.caption("âœ¨ Processed with entropy weighting")

    prompt = st.chat_input("Type your message here...")

    if "example_clicked" in st.session_state and st.session_state.example_clicked:
        prompt = st.session_state.example_clicked
        st.session_state.example_clicked = None

    if prompt:
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        with st.spinner("Processing and detecting eigenstates..."):
            # Tokenize user input with F-aware parallel processing
            # F=8 optimal for human-comprehension tasks
            tokens = tokenize_and_record_transitions_parallel(
                prompt,
                st.session_state.learned_tokens,
                F=8
            )

            # Process user input through EigenAI framework
            result = st.session_state.ai.process(prompt, verbose=False)

            # Generate token-based response (AI speaking from learned vocabulary)
            token_generated = generate_token_response(
                st.session_state.learned_tokens,
                prompt
            )

            # Learn tokens from generated response with F-aware parallel processing
            response_tokens = tokenize_and_record_transitions_parallel(
                token_generated,
                st.session_state.learned_tokens,
                F=8
            )

            # Feed the generated response back into itself for recursive understanding
            st.session_state.ai.process(token_generated, verbose=False)

            # Get updated state after recursive processing
            state = st.session_state.ai.get_state_summary()

            # Create response with eigenstate information
            full_response = f"**ğŸ”¢ Token-Generated Response:**\n{token_generated}\n\n"
            full_response += f"*({len(st.session_state.learned_tokens)} unique tokens learned)*\n\n"

            if entropy_mode:
                full_response += "âœ¨ *Entropy-weighted processing* | "

            if result['eigenstate']:
                full_response += f"âœ“ *Meta-eigenstate reached (iteration {result['iteration']}) | Framework strength: {state['M_context_norm']:.2f}*"
            else:
                full_response += f"â‹¯ *Building understanding (iteration {result['iteration']}) | Framework strength: {state['M_context_norm']:.2f}*"

            metrics = {
                "eigenstate": result['eigenstate'],
                "iteration": result['iteration'],
                "M_context_norm": state['M_context_norm'],
                "tokens": tokens,
                "vocab_size": len(st.session_state.learned_tokens),
                "entropy_weighted": entropy_mode
            }

            st.session_state.messages.append({
                "role": "assistant",
                "content": full_response,
                "metrics": metrics
            })
            st.session_state.metrics_history.append(metrics)

        st.rerun()

with col2:
    st.subheader("ğŸ“ˆ Understanding Evolution")

    state = st.session_state.ai.get_state_summary()

    st.metric("Total Inputs", state['iteration'])
    st.metric("Meta-Eigenstate", "âœ“ Reached" if state['eigenstate_reached'] else "â‹¯ Building")
    st.metric("Framework Strength", f"{state['M_context_norm']:.3f}")

    st.divider()

    st.subheader("ğŸ”§ Extraction Rules")
    st.caption("How the AI weights semantic components")

    if state['iteration'] > 0:
        latest_rules = st.session_state.ai.extraction_rules

        st.progress(min(latest_rules['L_weight'], 1.0), text=f"L (Lexical): {latest_rules['L_weight']:.3f}")
        st.progress(min(latest_rules['R_weight'], 1.0), text=f"R (Relational): {latest_rules['R_weight']:.3f}")
        st.progress(min(latest_rules['V_weight'], 1.0), text=f"V (Value): {latest_rules['V_weight']:.3f}")
        st.progress(min(latest_rules['context_influence'], 1.0), text=f"Context: {latest_rules['context_influence']:.3f}")
    else:
        st.info("Start chatting to see extraction rules evolve")

    st.divider()

    st.subheader("ğŸ’¡ Example Prompts")
    examples = [
        "Tell me about quantum mechanics",
        "What makes a good leader?",
        "How do neural networks learn?",
        "Explain Einstein's theory of relativity",
        "What is consciousness?"
    ]

    for ex in examples:
        if st.button(ex, key=ex, use_container_width=True):
            st.session_state.example_clicked = ex
            st.rerun()

    st.divider()

    if st.button("ğŸ”„ Reset Conversation", use_container_width=True):
        st.session_state.ai = RecursiveEigenAI(embedding_dim=64)
        st.session_state.messages = []
        st.session_state.metrics_history = []
        st.session_state.learned_tokens = {}
        st.rerun()

    st.divider()

    if st.session_state.learned_tokens:
        # Count tokens by classification
        time_like = sum(1 for t in st.session_state.learned_tokens.values() if t.get_classification() == 'time-like')
        space_like = sum(1 for t in st.session_state.learned_tokens.values() if t.get_classification() == 'space-like')
        light_like = sum(1 for t in st.session_state.learned_tokens.values() if t.get_classification() == 'light-like')
        unknown = sum(1 for t in st.session_state.learned_tokens.values() if t.get_classification() == 'unknown')

        st.caption(f"ğŸ“š **Vocabulary: {len(st.session_state.learned_tokens)} tokens**")
        st.caption(f"â±ï¸ Time-like (structural): {time_like}")
        st.caption(f"ğŸŒ Space-like (semantic): {space_like}")
        st.caption(f"ğŸ’« Light-like (relational): {light_like}")
        if unknown > 0:
            st.caption(f"â“ Unknown (learning): {unknown}")

st.sidebar.title("About EigenAI v1.2.0")
st.sidebar.markdown("""
### What is This?

This chatbot demonstrates **recursive self-modifying AI** that measures understanding through **eigenstate detection**.

### Key Concepts

**Semantic Triad (L, R, V)**:
- **L**: Subject/agent
- **R**: Predicate/verb
- **V**: Object/target
- **M**: Meta-understanding = L âŠ• R âŠ• V

**Eigenstate Detection**:
- Understanding = trajectory closure
- Fixed-point: Converged
- Periodic: Oscillating pattern
- None: Still learning

**Geometric Token Classification**:
From transition metric dsÂ² = SÂ² - CÂ²:
- â±ï¸ **Time-like** (S > C): Structural/sequential words (the, to, is)
- ğŸŒ **Space-like** (C > S): Semantic/content words (quantum, cat)
- ğŸ’« **Light-like** (C = S): Relational/transformational words (becomes, connects)

**Recursive Self-Modification**:
The AI changes its own processing framework based on what it learns, building genuine comprehension over time.

### New in v1.2.0

ğŸ§  **Context Accumulation**: Track accumulated context and measure relative information impact
ğŸ” **Novelty Detection**: Distinguish genuine learning from mere repetition
ğŸ¯ **Paradigm Shifts**: Detect phase transitions in understanding

### New in v1.1.0

âš¡ **F-Aware Parallelization**: 3-30Ã— speedup for token processing
ğŸ”¬ **Physics Metrics**: Momentum, velocity, phase tracking
ğŸ—„ï¸ **Database Optimization**: 100-1000Ã— speedup for batch operations

### Established in v1.0.0

âœ¨ **Entropy Weighting**: Weight semantic components by information density for 14.6Ã— better orthogonality

ğŸ“¦ **Install**: `pip install eigenai`

ğŸ’» **GitHub**: [InauguralPhysicist/EigenAI](https://github.com/InauguralPhysicist/EigenAI)

### How It Works

1. You send a message
2. AI extracts semantic triad (L, R, V)
3. Computes meta-understanding (M)
4. Detects eigenstate convergence
5. Updates extraction rules recursively
6. Generates response from learned tokens
""")
