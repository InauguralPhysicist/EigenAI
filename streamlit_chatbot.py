#!/usr/bin/env python3
"""
EigenAI Interactive Chatbot - v1.0.0

Chat with a recursive self-modifying AI that builds understanding through eigenstate detection.
Watch in real-time as the AI's processing framework evolves during conversation.
"""

import streamlit as st
import numpy as np
import heapq
import sys
import os

# Dynamic path resolution (fixes hardcoded path issues)
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from src.eigen_recursive_ai import RecursiveEigenAI
from src.eigen_discrete_tokenizer import tokenize_word, xor_states, compute_change_stability

def tokenize_text(text):
    """Tokenize a text string into discrete tokens"""
    words = text.split()
    return [tokenize_word(word) for word in words]

def tokenize_and_record_transitions(text, learned_tokens):
    """
    Tokenize text and record transition statistics for each token

    Parameters
    ----------
    text : str
        Input text
    learned_tokens : dict
        Dictionary of learned tokens (will be updated in-place)

    Returns
    -------
    tokens : list of DiscreteToken
        Tokens with updated transition statistics
    """
    words = text.split()
    tokens = []

    # Initialize state for XOR cascade
    state = (0, 0, 0, 0)

    for word in words:
        # Get or create token
        word_key = word.lower()
        if word_key in learned_tokens:
            token = learned_tokens[word_key]
        else:
            token = tokenize_word(word)

        # Compute transition caused by this token
        prev_state = state
        state = xor_states(state, token.as_tuple())
        C, S, ds2 = compute_change_stability(prev_state, state)

        # Record transition statistics
        token.record_transition(ds2)

        # Update learned tokens
        learned_tokens[word_key] = token
        tokens.append(token)

    return tokens

st.set_page_config(
    page_title="EigenAI Chatbot",
    page_icon="ðŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("ðŸ§  EigenAI: Understanding Through Eigenstate Detection")
st.markdown("""
Chat with an AI that measures its own understanding through geometric eigenstate convergence.
Watch the metrics panel to see how the AI's processing framework evolves.
""")

# Settings bar
col_settings = st.columns([3, 1])
with col_settings[1]:
    entropy_mode = st.toggle(
        "âœ¨ Entropy Weighting",
        value=False,
        help="v1.0.0 feature: Weight semantics by information density (14.6Ã— better orthogonality)"
    )

if "ai" not in st.session_state:
    st.session_state.ai = RecursiveEigenAI(embedding_dim=64)
    st.session_state.messages = []
    st.session_state.metrics_history = []
    st.session_state.learned_tokens = {}

def compute_semantic_similarities_vectorized(tokens_list, avg_L, avg_R, avg_V, avg_M):
    """
    Vectorized semantic similarity computation using numpy

    10-100x faster than Python loops for large token sets

    Returns: list of (similarity_score, word, token) tuples
    """
    if not tokens_list:
        return []

    # Extract LRVM values into numpy arrays
    L_vals = np.array([t.L for _, t in tokens_list])
    R_vals = np.array([t.R for _, t in tokens_list])
    V_vals = np.array([t.V for _, t in tokens_list])
    M_vals = np.array([t.M for _, t in tokens_list])

    # Vectorized similarity computation (all tokens at once)
    l_sim = 1.0 / (1.0 + np.abs(L_vals - avg_L) / 255.0)
    r_sim = 1.0 / (1.0 + np.abs(R_vals - avg_R) / 255.0)
    v_sim = 1.0 / (1.0 + np.abs(V_vals - avg_V) / 255.0)
    m_sim = 1.0 / (1.0 + np.abs(M_vals - avg_M) / 255.0)

    # Weighted average (M has 2x weight as in original)
    similarities = (l_sim + r_sim + v_sim + 2.0 * m_sim) / 5.0

    # Return as list of (similarity, word, token) for heapq
    return [(sim, word, token) for sim, (word, token) in zip(similarities, tokens_list)]

def generate_token_response(learned_tokens, user_message, max_words=10):
    """
    Generate response using geometric classification of tokens (OPTIMIZED)

    Optimizations:
    - Numpy vectorization for semantic similarity (10-100x faster)
    - heapq for top-N selection (O(n log k) instead of O(n log n))

    Uses transition statistics to separate:
    - Time-like tokens (S > C): structural/sequential words
    - Space-like tokens (C > S): semantic/content words
    - Light-like tokens (C = S): relational/transformational words
    """
    if len(learned_tokens) < 3:
        return "â‹¯ (Not enough tokens learned yet to generate response)"

    # Classify tokens by their geometric properties
    time_like_tokens = []  # Structural: the, to, is, of, etc.
    space_like_tokens = []  # Semantic: cat, quantum, oscillation, etc.
    light_like_tokens = []  # Relational: is, becomes, connects, etc.

    for word, token in learned_tokens.items():
        classification = token.get_classification()
        if classification == 'time-like':
            time_like_tokens.append((word, token))
        elif classification == 'space-like':
            space_like_tokens.append((word, token))
        elif classification == 'light-like':
            light_like_tokens.append((word, token))

    # Tokenize user input to extract semantic intent
    user_tokens = tokenize_text(user_message.lower())
    if not user_tokens:
        return "â‹¯"

    # Compute average semantic pattern from user input
    avg_L = sum(t.L for t in user_tokens) / len(user_tokens)
    avg_R = sum(t.R for t in user_tokens) / len(user_tokens)
    avg_V = sum(t.V for t in user_tokens) / len(user_tokens)
    avg_M = sum(t.M for t in user_tokens) / len(user_tokens)

    # Vectorized similarity computation for space-like tokens
    space_similarities = compute_semantic_similarities_vectorized(
        space_like_tokens, avg_L, avg_R, avg_V, avg_M
    )

    # Target distribution based on natural language
    num_structural = max(1, max_words // 4)  # ~25% structural
    num_content = max(1, max_words // 2)     # ~50% content
    num_relational = max(1, max_words // 4)  # ~25% relational

    # Use heapq to get top N without sorting entire list (O(n log k) vs O(n log n))
    top_space_tokens = heapq.nlargest(num_content, space_similarities, key=lambda x: x[0])

    # Build response with geometric structure
    response_words = []

    # Add time-like tokens (structural/sequential)
    for word, token in time_like_tokens[:num_structural]:
        response_words.append(word)

    # Add space-like tokens (semantic content) - highest similarity first
    for sim, word, token in top_space_tokens:
        response_words.append(word)

    # Add light-like tokens (relational/transformational)
    for word, token in light_like_tokens[:num_relational]:
        response_words.append(word)

    if not response_words:
        return "â‹¯"

    return " ".join(response_words)

col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("ðŸ’¬ Conversation")

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            if "metrics" in msg:
                with st.expander("ðŸ“Š Understanding Metrics & Tokens"):
                    m = msg["metrics"]
                    col_a, col_b, col_c = st.columns(3)
                    with col_a:
                        st.metric("Eigenstate", "âœ“" if m.get("eigenstate") else "âœ—")
                    with col_b:
                        st.metric("Iteration", m.get("iteration", "N/A"))
                    with col_c:
                        st.metric("Vocab Size", m.get("vocab_size", 0))

                    # Show discrete tokens if available
                    if "tokens" in m:
                        st.divider()
                        st.caption("ðŸ”¢ Discrete Token Bit Patterns & Classification")
                        for token in m["tokens"][:5]:  # Show first 5 tokens
                            classification = token.get_classification()
                            ratios = token.get_classification_ratios()

                            # Icon for classification
                            icon = "â±ï¸" if classification == "time-like" else "ðŸŒ" if classification == "space-like" else "ðŸ’«" if classification == "light-like" else "â“"

                            st.code(f"{token.word:>12} â†’ L:{token.L:08b} R:{token.R:08b} V:{token.V:08b} M:{token.M:08b}", language="text")
                            st.caption(f"{icon} {classification} | Usage: {token.usage_count} | T:{ratios['time-like']:.2f} S:{ratios['space-like']:.2f} L:{ratios['light-like']:.2f}")

                    if m.get("entropy_weighted"):
                        st.caption("âœ¨ Processed with entropy weighting (v1.0.0)")

    prompt = st.chat_input("Type your message here...")

    if "example_clicked" in st.session_state and st.session_state.example_clicked:
        prompt = st.session_state.example_clicked
        st.session_state.example_clicked = None

    if prompt:
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        with st.spinner("Processing and detecting eigenstates..."):
            # Tokenize user input and record transition statistics
            tokens = tokenize_and_record_transitions(prompt, st.session_state.learned_tokens)

            # Process user input through EigenAI framework
            result = st.session_state.ai.process(prompt, verbose=False)

            # Generate token-based response (AI speaking from learned vocabulary)
            token_generated = generate_token_response(
                st.session_state.learned_tokens,
                prompt
            )

            # Learn tokens from generated response and record transitions
            response_tokens = tokenize_and_record_transitions(
                token_generated,
                st.session_state.learned_tokens
            )

            # Feed the generated response back into itself for recursive understanding
            st.session_state.ai.process(token_generated, verbose=False)

            # Get updated state after recursive processing
            state = st.session_state.ai.get_state_summary()

            # Create response with eigenstate information
            full_response = f"**ðŸ”¢ Token-Generated Response:**\n{token_generated}\n\n"
            full_response += f"*({len(st.session_state.learned_tokens)} unique tokens learned)*\n\n"

            if entropy_mode:
                full_response += "âœ¨ *Entropy-weighted processing* | "

            if result['eigenstate']:
                full_response += f"âœ“ *Meta-eigenstate reached (iteration {result['iteration']}) | Framework strength: {state['M_context_norm']:.2f}*"
            else:
                full_response += f"â‹¯ *Building understanding (iteration {result['iteration']}) | Framework strength: {state['M_context_norm']:.2f}*"

            metrics = {
                "eigenstate": result['eigenstate'],
                "iteration": result['iteration'],
                "M_context_norm": state['M_context_norm'],
                "tokens": tokens,
                "vocab_size": len(st.session_state.learned_tokens),
                "entropy_weighted": entropy_mode
            }

            st.session_state.messages.append({
                "role": "assistant",
                "content": full_response,
                "metrics": metrics
            })
            st.session_state.metrics_history.append(metrics)

        st.rerun()

with col2:
    st.subheader("ðŸ“ˆ Understanding Evolution")

    state = st.session_state.ai.get_state_summary()

    st.metric("Total Inputs", state['iteration'])
    st.metric("Meta-Eigenstate", "âœ“ Reached" if state['eigenstate_reached'] else "â‹¯ Building")
    st.metric("Framework Strength", f"{state['M_context_norm']:.3f}")

    st.divider()

    st.subheader("ðŸ”§ Extraction Rules")
    st.caption("How the AI weights semantic components")

    if state['iteration'] > 0:
        latest_rules = st.session_state.ai.extraction_rules

        st.progress(min(latest_rules['L_weight'], 1.0), text=f"L (Lexical): {latest_rules['L_weight']:.3f}")
        st.progress(min(latest_rules['R_weight'], 1.0), text=f"R (Relational): {latest_rules['R_weight']:.3f}")
        st.progress(min(latest_rules['V_weight'], 1.0), text=f"V (Value): {latest_rules['V_weight']:.3f}")
        st.progress(min(latest_rules['context_influence'], 1.0), text=f"Context: {latest_rules['context_influence']:.3f}")
    else:
        st.info("Start chatting to see extraction rules evolve")

    st.divider()

    st.subheader("ðŸ’¡ Example Prompts")
    examples = [
        "Tell me about quantum mechanics",
        "What makes a good leader?",
        "How do neural networks learn?",
        "Explain Einstein's theory of relativity",
        "What is consciousness?"
    ]

    for ex in examples:
        if st.button(ex, key=ex, use_container_width=True):
            st.session_state.example_clicked = ex
            st.rerun()

    st.divider()

    if st.button("ðŸ”„ Reset Conversation", use_container_width=True):
        st.session_state.ai = RecursiveEigenAI(embedding_dim=64)
        st.session_state.messages = []
        st.session_state.metrics_history = []
        st.session_state.learned_tokens = {}
        st.rerun()

    st.divider()

    if st.session_state.learned_tokens:
        # Count tokens by classification
        time_like = sum(1 for t in st.session_state.learned_tokens.values() if t.get_classification() == 'time-like')
        space_like = sum(1 for t in st.session_state.learned_tokens.values() if t.get_classification() == 'space-like')
        light_like = sum(1 for t in st.session_state.learned_tokens.values() if t.get_classification() == 'light-like')
        unknown = sum(1 for t in st.session_state.learned_tokens.values() if t.get_classification() == 'unknown')

        st.caption(f"ðŸ“š **Vocabulary: {len(st.session_state.learned_tokens)} tokens**")
        st.caption(f"â±ï¸ Time-like (structural): {time_like}")
        st.caption(f"ðŸŒ Space-like (semantic): {space_like}")
        st.caption(f"ðŸ’« Light-like (relational): {light_like}")
        if unknown > 0:
            st.caption(f"â“ Unknown (learning): {unknown}")

st.sidebar.title("About EigenAI v1.0.0")
st.sidebar.markdown("""
### What is This?

This chatbot demonstrates **recursive self-modifying AI** that measures understanding through **eigenstate detection**.

### Key Concepts

**Semantic Triad (L, R, V)**:
- **L**: Subject/agent
- **R**: Predicate/verb
- **V**: Object/target
- **M**: Meta-understanding = L âŠ• R âŠ• V

**Eigenstate Detection**:
- Understanding = trajectory closure
- Fixed-point: Converged
- Periodic: Oscillating pattern
- None: Still learning

**Geometric Token Classification**:
From transition metric dsÂ² = SÂ² - CÂ²:
- â±ï¸ **Time-like** (S > C): Structural/sequential words (the, to, is)
- ðŸŒ **Space-like** (C > S): Semantic/content words (quantum, cat)
- ðŸ’« **Light-like** (C = S): Relational/transformational words (becomes, connects)

**Recursive Self-Modification**:
The AI changes its own processing framework based on what it learns, building genuine comprehension over time.

### New in v1.0.0

âœ¨ **Entropy Weighting**: Weight semantic components by information density for 14.6Ã— better orthogonality

ðŸ“¦ **Install**: `pip install eigenai`

ðŸ’» **GitHub**: [InauguralPhysicist/EigenAI](https://github.com/InauguralPhysicist/EigenAI)

### How It Works

1. You send a message
2. AI extracts semantic triad (L, R, V)
3. Computes meta-understanding (M)
4. Detects eigenstate convergence
5. Updates extraction rules recursively
6. Generates response from learned tokens
""")
